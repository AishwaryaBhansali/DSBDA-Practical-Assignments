{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a47920f-85a5-409e-b1bc-49e1e42577fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group A-7\n",
    "# Text Analytics\n",
    "# 1. Extract Sample document and apply following document preprocessing methods:\n",
    "# Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "# Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443b4c45-c983-46d6-9df1-f0ef8e877160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382627d3-f184-405e-97c6-160c3e316282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) enables machines to understand and interpret human language. It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data. Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning. NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"text.txt\")\n",
    "text = text_file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139a885b-47cb-4fa5-92e4-f720567652fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aishwarya\n",
      "[nltk_data]     Bhansali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Aishwarya\n",
      "[nltk_data]     Bhansali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64d0ef39-00bf-4335-9f86-b52eeb766d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) enables machines to understand and interpret human language.', 'It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data.', 'Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning.', 'NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tok = sent_tokenize(text)\n",
    "print(sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db6f1d91-cba9-461f-af61-b8ae9aeb4b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'It', 'bridges', 'the', 'gap', 'between', 'computers', 'and', 'humans', 'by', 'analyzing', ',', 'tokenizing', ',', 'and', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'and', 'lemmatization', 'play', 'a', 'crucial', 'role', 'in', 'simplifying', 'text', 'data', 'while', 'retaining', 'its', 'meaning', '.', 'NLP', 'is', 'widely', 'used', 'in', 'applications', 'such', 'as', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'search', 'engines', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tok = word_tokenize(text)\n",
    "print(word_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd71b4cf-2df4-46b3-bd4b-bd25142f0bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural : 1\n",
      "Language : 1\n",
      "Processing : 1\n",
      "( : 1\n",
      "NLP : 2\n",
      ") : 1\n",
      "enables : 1\n",
      "machines : 1\n",
      "to : 1\n",
      "understand : 1\n",
      "and : 5\n",
      "interpret : 1\n",
      "human : 1\n",
      "language : 1\n",
      ". : 4\n",
      "It : 1\n",
      "bridges : 1\n",
      "the : 1\n",
      "gap : 1\n",
      "between : 1\n",
      "computers : 1\n",
      "humans : 1\n",
      "by : 1\n",
      "analyzing : 1\n",
      ", : 4\n",
      "tokenizing : 1\n",
      "processing : 1\n",
      "textual : 1\n",
      "data : 2\n",
      "Techniques : 1\n",
      "like : 1\n",
      "stemming : 1\n",
      "lemmatization : 1\n",
      "play : 1\n",
      "a : 1\n",
      "crucial : 1\n",
      "role : 1\n",
      "in : 2\n",
      "simplifying : 1\n",
      "text : 1\n",
      "while : 1\n",
      "retaining : 1\n",
      "its : 1\n",
      "meaning : 1\n",
      "is : 1\n",
      "widely : 1\n",
      "used : 1\n",
      "applications : 1\n",
      "such : 1\n",
      "as : 1\n",
      "chatbots : 1\n",
      "sentiment : 1\n",
      "analysis : 1\n",
      "search : 1\n",
      "engines : 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq = FreqDist(word_tok)\n",
    "all_words = freq.items()\n",
    "for word, count in all_words:\n",
    "    print(f\"{word} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fce130-b2e4-4a5a-8962-779def384817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4f7742-feb0-48b3-a7e4-c7585cbd6e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World??\"\n",
    "text_clean = \"\".join(char for char in text if char not in string.punctuation)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db34dd5e-4249-425d-8a67-b8558ea1ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text :  ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'It', 'bridges', 'the', 'gap', 'between', 'computers', 'and', 'humans', 'by', 'analyzing', ',', 'tokenizing', ',', 'and', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'and', 'lemmatization', 'play', 'a', 'crucial', 'role', 'in', 'simplifying', 'text', 'data', 'while', 'retaining', 'its', 'meaning', '.', 'NLP', 'is', 'widely', 'used', 'in', 'applications', 'such', 'as', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'search', 'engines', '.']\n",
      "Without Stopwords text :  ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'understand', 'interpret', 'human', 'language', '.', 'bridges', 'gap', 'computers', 'humans', 'analyzing', ',', 'tokenizing', ',', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'lemmatization', 'play', 'crucial', 'role', 'simplifying', 'text', 'data', 'retaining', 'meaning', '.', 'NLP', 'widely', 'used', 'applications', 'chatbots', ',', 'sentiment', 'analysis', ',', 'search', 'engines', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "print(\"Original text : \", word_tok)\n",
    "\n",
    "removed = [char for char in word_tok if char.lower() not in stopwords_list]\n",
    "print(\"Without Stopwords text : \", removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7702f3-e625-4aab-9b61-12bdacc2a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('interpret', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('bridges', 'VBZ'), ('the', 'DT'), ('gap', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('by', 'IN'), ('analyzing', 'VBG'), (',', ','), ('tokenizing', 'VBG'), (',', ','), ('and', 'CC'), ('processing', 'VBG'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.'), ('Techniques', 'NNS'), ('like', 'IN'), ('stemming', 'VBG'), ('and', 'CC'), ('lemmatization', 'NN'), ('play', 'VBP'), ('a', 'DT'), ('crucial', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('simplifying', 'VBG'), ('text', 'JJ'), ('data', 'NNS'), ('while', 'IN'), ('retaining', 'VBG'), ('its', 'PRP$'), ('meaning', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('is', 'VBZ'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('applications', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('chatbots', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('and', 'CC'), ('search', 'NN'), ('engines', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "postag = pos_tag(word_tok)\n",
    "print(postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d102986-b225-4f74-8351-21d05d8e3fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "word = ['run', 'running', 'runner']\n",
    "for x in word:\n",
    "    print(ps.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fc689d7-e0a8-4def-8768-5b25aa2b44be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Aishwarya\n",
      "[nltk_data]     Bhansali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5abd78e-b2f2-4870-be87-827cbd45793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aishwarya\n",
      "[nltk_data]     Bhansali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2349e1d5-2609-4897-aa09-0d94a004761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "running\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "text = [\"run\", \"running\", \"runner\"]\n",
    "for x in text:\n",
    "#lem = [lem.lemmatize(word) for word in word_tok]\n",
    "    print(lem.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c1c0f6f-fca2-4665-9c36-e5d0389ed92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\study material\\dsbd\\venv\\lib\\site-packages (1.6.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\study material\\dsbd\\venv\\lib\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\study material\\dsbd\\venv\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\study material\\dsbd\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\study material\\dsbd\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1b8adff-2926-4988-b96a-e40777a40350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 57 stored elements and shape (68, 48)>\n",
      "  Coords\tValues\n",
      "  (0, 27)\t1.0\n",
      "  (1, 22)\t1.0\n",
      "  (2, 30)\t1.0\n",
      "  (4, 28)\t1.0\n",
      "  (6, 12)\t1.0\n",
      "  (7, 25)\t1.0\n",
      "  (8, 42)\t1.0\n",
      "  (9, 44)\t1.0\n",
      "  (10, 2)\t1.0\n",
      "  (11, 18)\t1.0\n",
      "  (12, 15)\t1.0\n",
      "  (13, 22)\t1.0\n",
      "  (15, 20)\t1.0\n",
      "  (16, 6)\t1.0\n",
      "  (17, 41)\t1.0\n",
      "  (18, 14)\t1.0\n",
      "  (19, 5)\t1.0\n",
      "  (20, 9)\t1.0\n",
      "  (21, 2)\t1.0\n",
      "  (22, 16)\t1.0\n",
      "  (23, 7)\t1.0\n",
      "  (24, 1)\t1.0\n",
      "  (26, 43)\t1.0\n",
      "  (28, 2)\t1.0\n",
      "  (29, 30)\t1.0\n",
      "  :\t:\n",
      "  (38, 29)\t1.0\n",
      "  (40, 10)\t1.0\n",
      "  (41, 32)\t1.0\n",
      "  (42, 17)\t1.0\n",
      "  (43, 35)\t1.0\n",
      "  (44, 39)\t1.0\n",
      "  (45, 11)\t1.0\n",
      "  (46, 46)\t1.0\n",
      "  (47, 31)\t1.0\n",
      "  (48, 21)\t1.0\n",
      "  (49, 26)\t1.0\n",
      "  (51, 28)\t1.0\n",
      "  (52, 19)\t1.0\n",
      "  (53, 47)\t1.0\n",
      "  (54, 45)\t1.0\n",
      "  (55, 17)\t1.0\n",
      "  (56, 3)\t1.0\n",
      "  (57, 37)\t1.0\n",
      "  (58, 4)\t1.0\n",
      "  (59, 8)\t1.0\n",
      "  (61, 34)\t1.0\n",
      "  (62, 0)\t1.0\n",
      "  (64, 2)\t1.0\n",
      "  (65, 33)\t1.0\n",
      "  (66, 13)\t1.0\n",
      "['analysis' 'analyzing' 'and' 'applications' 'as' 'between' 'bridges' 'by'\n",
      " 'chatbots' 'computers' 'crucial' 'data' 'enables' 'engines' 'gap' 'human'\n",
      " 'humans' 'in' 'interpret' 'is' 'it' 'its' 'language' 'lemmatization'\n",
      " 'like' 'machines' 'meaning' 'natural' 'nlp' 'play' 'processing'\n",
      " 'retaining' 'role' 'search' 'sentiment' 'simplifying' 'stemming' 'such'\n",
      " 'techniques' 'text' 'textual' 'the' 'to' 'tokenizing' 'understand' 'used'\n",
      " 'while' 'widely']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "tfidf = tf.fit_transform(word_tok)\n",
    "print(tfidf)\n",
    "feature_names = tf.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0c6ca-00cd-4c43-91e3-481e916fc353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
